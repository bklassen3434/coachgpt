base_model: mistralai/Mistral-7B-v0.1
model_type: mistral
tokenizer_use_fast: true
load_in_4bit: true

datasets:
  - path: training_data/softball_qa_chatml.jsonl
    type: chat

dataset_prepared_path: last_run_prepared
output_dir: ./softball-lora-out
adapter: lora

lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj"]

sequence_len: 2048
sample_packing: true
train_on_inputs: false
group_by_length: true

num_epochs: 3
micro_batch_size: 4
gradient_accumulation_steps: 4

learning_rate: 2e-4
lr_scheduler: cosine
weight_decay: 0.01
warmup_steps: 50

eval_steps: 10
save_strategy: "epoch"
logging_steps: 1
